# ðŸ› ï¸ Steps to Create an Azure Data Pipeline in ADF

---

## ðŸ“Œ Introduction
An **Azure Data Pipeline** in **Azure Data Factory (ADF)** is a workflow that **moves and transforms data** between sources and destinations.  
The pipeline can be built using the **ADF Studio UI** or through **ARM templates / SDKs / CLI**.

---

## ðŸ”‘ Steps to Create a Data Pipeline

### 1ï¸âƒ£ **Create an Azure Data Factory**
- Go to **Azure Portal** â†’ Search for **Data Factory** â†’ Click **Create**.
- Select:
  - **Subscription** (billing scope).
  - **Resource Group** (container for resources).
  - **Region**.
  - **Name** for Data Factory.
- Click **Review + Create** â†’ Deploy.

---

### 2ï¸âƒ£ **Open ADF Studio**
- After deployment, open **ADF Studio** (web-based UI).
- This is where you create pipelines, dataflows, and monitor activities.

---

### 3ï¸âƒ£ **Create Linked Services**
- Go to **Manage â†’ Linked Services**.
- Create connections to **data sources** (e.g., Azure Blob, SQL DB, Amazon S3, on-prem DB).
- Provide **authentication details** (keys, connection strings, service principals).

---

### 4ï¸âƒ£ **Define Datasets**
- Go to **Author â†’ Datasets**.
- Create **datasets** representing input (source) and output (sink) data.
  - Example: Blob CSV dataset, SQL table dataset.
- Each dataset is tied to a **Linked Service**.

---

### 5ï¸âƒ£ **Build the Pipeline**
- Go to **Author â†’ Pipelines** â†’ **New Pipeline**.
- Add activities:
  - **Copy Activity** â†’ Move data from source to sink.
  - **Data Flow** â†’ Perform transformations (join, filter, aggregate).
  - **Control Activities** â†’ (If Condition, ForEach, Execute Pipeline).
- Connect datasets to activities.

---

### 6ï¸âƒ£ **Configure Parameters (Optional)**
- Define **pipeline parameters** for dynamic values (file paths, table names).
- Helps in **reusability and automation**.

---

### 7ï¸âƒ£ **Set Triggers**
- Go to **Triggers** â†’ Choose how the pipeline runs:
  - **Manual Run** â†’ Trigger instantly.
  - **Schedule Trigger** â†’ Run at fixed intervals (e.g., daily 9 AM).
  - **Tumbling Window Trigger** â†’ Run in batch windows.
  - **Event Trigger** â†’ Run when file arrives in Blob Storage.

---

### 8ï¸âƒ£ **Publish Pipeline**
- Click **Publish All** to save changes.
- This makes the pipeline active and ready to run.

---

### 9ï¸âƒ£ **Run & Monitor Pipeline**
- Trigger the pipeline manually or wait for scheduled execution.
- Go to **Monitor Tab**:
  - Track **pipeline runs**.
  - Check **success/failure** status.
  - Review **logs and performance metrics**.

---

## ðŸ“Š Pipeline Creation Workflow (Visual)

```

+-----------------+       +-------------------+       +-----------------+
|  Data Factory   | --->  |   Linked Services | --->  |     Datasets    |
+-----------------+       +-------------------+       +-----------------+
|
v
+-----------+
| Pipeline  |
| Activities|
+-----------+
|
v
+---------------+
|   Triggers    |
+---------------+
|
v
+----------------+
|   Monitoring   |
+----------------+

```

---

## ðŸš€ Example: Copy Data from Blob to SQL
1. Create a **Linked Service** for Blob & SQL DB.  
2. Create **Datasets** for Blob CSV file & SQL table.  
3. Create a **Pipeline** with a **Copy Activity**.  
4. Add a **Schedule Trigger** to run daily.  
5. **Publish & Monitor** execution.  

---

## ðŸŽ¯ Key Takeaways
- ADF pipelines involve **Linked Services, Datasets, Pipelines, Activities, Triggers**.  
- Pipelines can be **scheduled, event-driven, or manual**.  
- Monitoring ensures **visibility and debugging**.  

---
