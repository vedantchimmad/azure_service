# ‚òÅÔ∏è Azure Data Factory (ADF)

###  üìå What is ADF?
**Azure Data Factory** is a **cloud-based ETL (Extract, Transform, Load) and data integration service** provided by Microsoft Azure.  
It lets you create **data pipelines** to move and transform data across various sources at scale.

---

### üß± Key Components

| Component        | Description                                                            |
|------------------|------------------------------------------------------------------------|
| Pipeline         | A group of activities that perform a task                              |
| Activity         | An action like Copy Data, Execute SQL, or Data Flow                    |
| Dataset          | Metadata that points to input/output data sources                      |
| Linked Service   | Connection info for data sources (e.g., Azure Blob, SQL DB, etc.)       |
| Trigger          | Schedules or event-based starts for pipelines                          |
| Integration Runtime | Execution engine that runs your pipeline activities                  |

---

### üîÑ What Can ADF Do?

‚úÖ Copy data from 100+ sources  
‚úÖ Run transformations using **Mapping Data Flows** (code-free) or external compute (e.g., Databricks, HDInsight, SQL)  
‚úÖ Schedule and orchestrate workflows  
‚úÖ Monitor data pipelines with logs and alerts  
‚úÖ Handle on-premise and cloud data with hybrid integration

---

### üì§ Example Use Case

üîπ **Copy data** from on-prem SQL Server to Azure Data Lake  
üîπ **Clean and transform** it using a Data Flow  
üîπ **Load it** into Azure Synapse Analytics for reporting

---

### üõ†Ô∏è Sample JSON Pipeline (Simplified)
```json
{
  "name": "CopySalesDataPipeline",
  "properties": {
    "activities": [
      {
        "name": "CopyFromBlobToSQL",
        "type": "Copy",
        "inputs": [{ "referenceName": "BlobSalesData" }],
        "outputs": [{ "referenceName": "SQLSalesTable" }],
        "typeProperties": {
          "source": { "type": "DelimitedTextSource" },
          "sink": { "type": "SqlSink" }
        }
      }
    ]
  }
}
```

### üß© Azure Data Factory (ADF) ‚Äì Core Components

Azure Data Factory is a cloud-based data integration service that allows you to create, schedule, and orchestrate data pipelines.

### üß± 1. Pipeline

* A **pipeline** is a container for one or more activities.



* A pipeline is a **logical container** for a sequence of activities.

```json
{
  "name": "MyDataPipeline",
  "properties": {
    "activities": []
  }
}
```

---

### üèó 2. Activity

An **activity** is a single task, such as copying data or executing a SQL script.

**Common activity types**:
- `CopyActivity`
- `DataFlowActivity`
- `ExecutePipelineActivity`
- `NotebookActivity`
```json
{
  "name": "CopyBlobToSQL",
  "type": "Copy",
  "inputs": ["BlobInput"],
  "outputs": ["SqlOutput"],
  "typeProperties": {
    "source": { "type": "DelimitedTextSource" },
    "sink": { "type": "SqlSink" }
  }
}
```

---

### üìÅ 3. Dataset

* A **dataset** represents input or output data ‚Äî like a file or table.
* A dataset defines **metadata** pointing to the data you want to use.

```json
{
  "name": "BlobSalesData",
  "type": "Dataset",
  "properties": {
    "linkedServiceName": { "referenceName": "AzureBlobStorage" },
    "type": "DelimitedText",
    "typeProperties": {
      "location": { "type": "AzureBlobStorageLocation", "folderPath": "raw/sales" },
      "columnDelimiter": ",",
      "firstRowAsHeader": true
    }
  }
}
```

---

### üîó 4. Linked Service

A **linked service** defines the connection to a data source or compute resource.

```json
{
  "name": "AzureSQLLinkedService",
  "type": "LinkedService",
  "properties": {
    "type": "AzureSqlDatabase",
    "typeProperties": {
      "connectionString": "Server=tcp:myserver.database.windows.net;Database=mydb;..."
    }
  }
}
```

---

### ‚è∞ 5. Trigger

* A **trigger** defines when a pipeline is executed.
* A trigger defines **when and how** a pipeline should execute.


‚è± **Types**:
- Schedule Trigger
- Tumbling Window
- Event Trigger

```json
{
  "name": "DailyTrigger",
  "type": "ScheduleTrigger",
  "properties": {
    "recurrence": {
      "frequency": "Day",
      "interval": 1
    }
  }
}
```

---

### üñ•Ô∏è 6. Integration Runtime (IR)

IR is the **compute environment** for running activities.

### Types:
- `Azure` (default for cloud)
- `Self-hosted` (on-premise)
- `Azure-SSIS` (for SSIS packages)

```json
{
  "name": "AzureIR",
  "type": "IntegrationRuntimeReference",
  "referenceName": "AutoResolveIntegrationRuntime"
}
```

---

### üßÆ 7. Parameters

**Parameters** allow pipelines and datasets to be dynamic.

```json
"parameters": {
  "filename": {
    "type": "String"
  }
}
```

---

### üîÑ 8. Data Flow

**Data Flows** provide visual, drag-and-drop transformation capabilities.

```json
{
  "name": "CleanSalesDataFlow",
  "type": "MappingDataFlow",
  "typeProperties": {
    "sources": [...],
    "transformations": [...],
    "sinks": [...]
  }
}
```

---

### üìä Summary Table

| Component           | Purpose                                 |
|---------------------|------------------------------------------|
| Pipeline            | Container of activities                  |
| Activity            | Task like Copy, Lookup, Execute Notebook |
| Dataset             | Input/output data descriptor             |
| Linked Service      | Data source connection                   |
| Trigger             | Defines execution schedule               |
| Integration Runtime | Compute engine for pipeline execution    |
| Parameters          | Makes logic dynamic and reusable         |
| Data Flow           | GUI-based data transformation logic      |

---


