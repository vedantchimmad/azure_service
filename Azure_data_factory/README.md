# â˜ï¸ Azure Data Factory (ADF)

###  ğŸ“Œ What is ADF?
**Azure Data Factory** is a **cloud-based ETL (Extract, Transform, Load) and data integration service** provided by Microsoft Azure.  
It lets you create **data pipelines** to move and transform data across various sources at scale.

---

### ğŸ§± Key Components

| Component        | Description                                                            |
|------------------|------------------------------------------------------------------------|
| Pipeline         | A group of activities that perform a task                              |
| Activity         | An action like Copy Data, Execute SQL, or Data Flow                    |
| Dataset          | Metadata that points to input/output data sources                      |
| Linked Service   | Connection info for data sources (e.g., Azure Blob, SQL DB, etc.)       |
| Trigger          | Schedules or event-based starts for pipelines                          |
| Integration Runtime | Execution engine that runs your pipeline activities                  |

---

### ğŸ”„ What Can ADF Do?

âœ… Copy data from 100+ sources  
âœ… Run transformations using **Mapping Data Flows** (code-free) or external compute (e.g., Databricks, HDInsight, SQL)  
âœ… Schedule and orchestrate workflows  
âœ… Monitor data pipelines with logs and alerts  
âœ… Handle on-premise and cloud data with hybrid integration

---

### ğŸ“¤ Example Use Case

ğŸ”¹ **Copy data** from on-prem SQL Server to Azure Data Lake  
ğŸ”¹ **Clean and transform** it using a Data Flow  
ğŸ”¹ **Load it** into Azure Synapse Analytics for reporting

---

### ğŸ› ï¸ Sample JSON Pipeline (Simplified)
```json
{
  "name": "CopySalesDataPipeline",
  "properties": {
    "activities": [
      {
        "name": "CopyFromBlobToSQL",
        "type": "Copy",
        "inputs": [{ "referenceName": "BlobSalesData" }],
        "outputs": [{ "referenceName": "SQLSalesTable" }],
        "typeProperties": {
          "source": { "type": "DelimitedTextSource" },
          "sink": { "type": "SqlSink" }
        }
      }
    ]
  }
}
```

### ğŸ§© Azure Data Factory (ADF) â€“ Core Components

Azure Data Factory is a cloud-based data integration service that allows you to create, schedule, and orchestrate data pipelines.

### ğŸ§± 1. Pipeline

* A **pipeline** is a container for one or more activities.



* A pipeline is a **logical container** for a sequence of activities.

```json
{
  "name": "MyDataPipeline",
  "properties": {
    "activities": []
  }
}
```

---

### ğŸ— 2. Activity

An **activity** is a single task, such as copying data or executing a SQL script.

**Common activity types**:
- `CopyActivity`
- `DataFlowActivity`
- `ExecutePipelineActivity`
- `NotebookActivity`
```json
{
  "name": "CopyBlobToSQL",
  "type": "Copy",
  "inputs": ["BlobInput"],
  "outputs": ["SqlOutput"],
  "typeProperties": {
    "source": { "type": "DelimitedTextSource" },
    "sink": { "type": "SqlSink" }
  }
}
```

---

### ğŸ“ 3. Dataset

* A **dataset** represents input or output data â€” like a file or table.
* A dataset defines **metadata** pointing to the data you want to use.

```json
{
  "name": "BlobSalesData",
  "type": "Dataset",
  "properties": {
    "linkedServiceName": { "referenceName": "AzureBlobStorage" },
    "type": "DelimitedText",
    "typeProperties": {
      "location": { "type": "AzureBlobStorageLocation", "folderPath": "raw/sales" },
      "columnDelimiter": ",",
      "firstRowAsHeader": true
    }
  }
}
```

---

### ğŸ”— 4. Linked Service

A **linked service** defines the connection to a data source or compute resource.

```json
{
  "name": "AzureSQLLinkedService",
  "type": "LinkedService",
  "properties": {
    "type": "AzureSqlDatabase",
    "typeProperties": {
      "connectionString": "Server=tcp:myserver.database.windows.net;Database=mydb;..."
    }
  }
}
```

---

### â° 5. Trigger

* A **trigger** defines when a pipeline is executed.
* A trigger defines **when and how** a pipeline should execute.


â± **Types**:
- **Schedule Trigger** : Runs pipelines at **specific intervals** (e.g., hourly, daily).
- **Tumbling Window** : Processes data in **fixed-size, non-overlapping intervals**. Supports **retry, concurrency, and dependency** chaining.
- **Event Trigger** : Triggers a pipeline based on **blob creation or deletion events** in Azure Storage.
- **Manual Trigger** : Pipelines can be executed manually **without a defined trigger**

| Feature                     | Schedule Trigger        | Tumbling Window Trigger             |
|-----------------------------|-------------------------|-------------------------------------|
| Time Window Overlap         | Can overlap             | Fixed, non-overlapping              |
| Dependency Support          | âŒ No                    | âœ… Yes                               |
| Backfill Support            | âŒ No                    | âœ… Yes                               |
| Retry on Failure            | âŒ No                    | âœ… Yes                               |
| Runs if Previous Fails?     | âœ… Yes                   | âŒ No (unless retried/backfilled)    |
| Use Case                    | Simple recurring tasks  | Time-partitioned ingestion          |

```json
{
  "name": "DailyTrigger",
  "type": "ScheduleTrigger",
  "properties": {
    "recurrence": {
      "frequency": "Day",
      "interval": 1
    }
  }
}
```

---

### ğŸ–¥ï¸ 6. Integration Runtime (IR)

IR is the **compute environment** for running activities.

### Types:
- `Azure` (default for cloud)
- `Self-hosted` (on-premise)
- `Azure-SSIS` (for SSIS packages)

```json
{
  "name": "AzureIR",
  "type": "IntegrationRuntimeReference",
  "referenceName": "AutoResolveIntegrationRuntime"
}
```

---

### ğŸ§® 7. Parameters

**Parameters** allow pipelines and datasets to be dynamic.

```json
"parameters": {
  "filename": {
    "type": "String"
  }
}
```

---

### ğŸ”„ 8. Data Flow

* **Data Flows** provide visual, drag-and-drop transformation capabilities.
* A **Data Flow** is a **graphical data transformation** component in ADF

### ğŸ”„ Common Transformations

| Transformation    | Purpose                                  |
|-------------------|------------------------------------------|
| Filter            | Include/exclude rows based on conditions |
| Join              | Combine data from two sources            |
| Derived Column    | Create new columns using expressions     |
| Aggregate         | Perform groupings and aggregations       |
| Conditional Split | Route data to different paths            |
| Pivot/Unpivot     | Reshape tabular data                     |
| Surrogate Key     | Add unique identifiers                   |
| Lookup            | Reference data from another source       |

```json
{
  "name": "CleanSalesDataFlow",
  "type": "MappingDataFlow",
  "typeProperties": {
    "sources": [...],
    "transformations": [...],
    "sinks": [...]
  }
}
```

---

### ğŸ“Š Summary Table

| Component           | Purpose                                 |
|---------------------|------------------------------------------|
| Pipeline            | Container of activities                  |
| Activity            | Task like Copy, Lookup, Execute Notebook |
| Dataset             | Input/output data descriptor             |
| Linked Service      | Data source connection                   |
| Trigger             | Defines execution schedule               |
| Integration Runtime | Compute engine for pipeline execution    |
| Parameters          | Makes logic dynamic and reusable         |
| Data Flow           | GUI-based data transformation logic      |

---
## ğŸ§© Parameterization in Azure Data Factory (ADF)

**Parameterization** in ADF allows you to create **flexible, reusable pipelines, data flows, datasets, and linked services** by passing values at runtime.

---

### ğŸ”‘ Why Parameterization?

- **Reuse components** with different values (e.g., filenames, table names)
- **Simplify maintenance** by avoiding hardcoding
- **Enable dynamic pipeline execution**

---

### ğŸ¯ Where You Can Use Parameters

| Component         | Supports Parameters? | Example Use Case                         |
|------------------|----------------------|------------------------------------------|
| Pipeline          | âœ… Yes               | File name, table name, load type         |
| Dataset           | âœ… Yes               | File path, table name                    |
| Linked Service    | âœ… Yes               | Database name, connection string         |
| Data Flow         | âœ… Yes               | Source filters, derived columns          |
| Triggers          | âœ… Yes               | Runtime date passed to pipeline          |

---

### ğŸ§ª Example: Pipeline Parameter

```json
"parameters": {
  "FileName": {
    "type": "String"
  }
}
```

#### ğŸ‘‡ Usage in Activity
```json
"dataset": {
  "referenceName": "InputDataset",
  "parameters": {
    "fileName": "@pipeline().parameters.FileName"
  }
}
```

---

### ğŸ“¦ Dataset Parameter Example

```json
{
  "name": "InputDataset",
  "properties": {
    "parameters": {
      "fileName": {
        "type": "String"
      }
    },
    "type": "DelimitedText",
    "typeProperties": {
      "location": {
        "type": "AzureBlobStorageLocation",
        "fileName": "@dataset().fileName",
        "folderPath": "input"
      }
    }
  }
}
```

---

### ğŸ”„ Data Flow Parameters

### ğŸ§® Define a parameter:
```json
"parameters": {
  "sourceTable": {
    "type": "string"
  }
}
```

#### ğŸ“Œ Use in a Source transformation:
```plaintext
SELECT * FROM @{sourceTable}
```

---

### ğŸ•’ Trigger-Time Parameterization

```json
"pipelines": [
  {
    "parameters": {
      "runDate": "@trigger().startTime"
    }
  }
]
```

---

### ğŸ› ï¸ Expression Language Support

ADF uses **Dynamic Content Expressions** (similar to Azure Logic Apps):

- `@pipeline().parameters.paramName`
- `@dataset().paramName`
- `@activity('CopyActivity').output`
- `@utcNow()`
- `@concat()`, `@formatDateTime()`, etc.

---

### âœ… Best Practices

- Use meaningful parameter names (e.g., `sourceFilePath`, `runDate`)
- Combine with **expressions** for maximum flexibility
- Use in **looping constructs** (e.g., `ForEach`) to drive dynamic behavior
- Keep **defaults** for local testing

---


