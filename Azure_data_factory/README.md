# ‚òÅÔ∏è Azure Data Factory (ADF)

###  üìå What is ADF?
**Azure Data Factory** is a **cloud-based ETL (Extract, Transform, Load) and data integration service** provided by Microsoft Azure.  
It lets you create **data pipelines** to move and transform data across various sources at scale.

---

### üß± Key Components

| Component        | Description                                                            |
|------------------|------------------------------------------------------------------------|
| Pipeline         | A group of activities that perform a task                              |
| Activity         | An action like Copy Data, Execute SQL, or Data Flow                    |
| Dataset          | Metadata that points to input/output data sources                      |
| Linked Service   | Connection info for data sources (e.g., Azure Blob, SQL DB, etc.)       |
| Trigger          | Schedules or event-based starts for pipelines                          |
| Integration Runtime | Execution engine that runs your pipeline activities                  |

---

### üîÑ What Can ADF Do?

‚úÖ Copy data from 100+ sources  
‚úÖ Run transformations using **Mapping Data Flows** (code-free) or external compute (e.g., Databricks, HDInsight, SQL)  
‚úÖ Schedule and orchestrate workflows  
‚úÖ Monitor data pipelines with logs and alerts  
‚úÖ Handle on-premise and cloud data with hybrid integration

---

### üì§ Example Use Case

üîπ **Copy data** from on-prem SQL Server to Azure Data Lake  
üîπ **Clean and transform** it using a Data Flow  
üîπ **Load it** into Azure Synapse Analytics for reporting

---

### üõ†Ô∏è Sample JSON Pipeline (Simplified)
```json
{
  "name": "CopySalesDataPipeline",
  "properties": {
    "activities": [
      {
        "name": "CopyFromBlobToSQL",
        "type": "Copy",
        "inputs": [{ "referenceName": "BlobSalesData" }],
        "outputs": [{ "referenceName": "SQLSalesTable" }],
        "typeProperties": {
          "source": { "type": "DelimitedTextSource" },
          "sink": { "type": "SqlSink" }
        }
      }
    ]
  }
}
```

### üß© Azure Data Factory (ADF) ‚Äì Core Components

Azure Data Factory is a cloud-based data integration service that allows you to create, schedule, and orchestrate data pipelines.

### üß± 1. Pipeline

* A **pipeline** is a container for one or more activities.



* A pipeline is a **logical container** for a sequence of activities.

```json
{
  "name": "MyDataPipeline",
  "properties": {
    "activities": []
  }
}
```

---

### üèó 2. Activity

An **activity** is a single task, such as copying data or executing a SQL script.

**Common activity types**:
- `CopyActivity`
- `DataFlowActivity`
- `ExecutePipelineActivity`
- `NotebookActivity`
```json
{
  "name": "CopyBlobToSQL",
  "type": "Copy",
  "inputs": ["BlobInput"],
  "outputs": ["SqlOutput"],
  "typeProperties": {
    "source": { "type": "DelimitedTextSource" },
    "sink": { "type": "SqlSink" }
  }
}
```

---

### üìÅ 3. Dataset

* A **dataset** represents input or output data ‚Äî like a file or table.
* A dataset defines **metadata** pointing to the data you want to use.

```json
{
  "name": "BlobSalesData",
  "type": "Dataset",
  "properties": {
    "linkedServiceName": { "referenceName": "AzureBlobStorage" },
    "type": "DelimitedText",
    "typeProperties": {
      "location": { "type": "AzureBlobStorageLocation", "folderPath": "raw/sales" },
      "columnDelimiter": ",",
      "firstRowAsHeader": true
    }
  }
}
```

---

### üîó 4. Linked Service

A **linked service** defines the connection to a data source or compute resource.

```json
{
  "name": "AzureSQLLinkedService",
  "type": "LinkedService",
  "properties": {
    "type": "AzureSqlDatabase",
    "typeProperties": {
      "connectionString": "Server=tcp:myserver.database.windows.net;Database=mydb;..."
    }
  }
}
```

---

### ‚è∞ 5. Trigger

* A **trigger** defines when a pipeline is executed.
* A trigger defines **when and how** a pipeline should execute.


‚è± **Types**:
- **Schedule Trigger** : Runs pipelines at **specific intervals** (e.g., hourly, daily).
- **Tumbling Window** : Processes data in **fixed-size, non-overlapping intervals**. Supports **retry, concurrency, and dependency** chaining.
- **Event Trigger** : Triggers a pipeline based on **blob creation or deletion events** in Azure Storage.
- **Manual Trigger** : Pipelines can be executed manually **without a defined trigger**

| Feature                     | Schedule Trigger        | Tumbling Window Trigger             |
|-----------------------------|-------------------------|-------------------------------------|
| Time Window Overlap         | Can overlap             | Fixed, non-overlapping              |
| Dependency Support          | ‚ùå No                    | ‚úÖ Yes                               |
| Backfill Support            | ‚ùå No                    | ‚úÖ Yes                               |
| Retry on Failure            | ‚ùå No                    | ‚úÖ Yes                               |
| Runs if Previous Fails?     | ‚úÖ Yes                   | ‚ùå No (unless retried/backfilled)    |
| Use Case                    | Simple recurring tasks  | Time-partitioned ingestion          |

```json
{
  "name": "DailyTrigger",
  "type": "ScheduleTrigger",
  "properties": {
    "recurrence": {
      "frequency": "Day",
      "interval": 1
    }
  }
}
```

---

### üñ•Ô∏è 6. Integration Runtime (IR)

IR is the **compute environment** for running activities.

### Types:
- `Azure` (default for cloud)
- `Self-hosted` (on-premise)
- `Azure-SSIS` (for SSIS packages)

```json
{
  "name": "AzureIR",
  "type": "IntegrationRuntimeReference",
  "referenceName": "AutoResolveIntegrationRuntime"
}
```

---

### üßÆ 7. Parameters

**Parameters** allow pipelines and datasets to be dynamic.

```json
"parameters": {
  "filename": {
    "type": "String"
  }
}
```

---

### üîÑ 8. Data Flow

* **Data Flows** provide visual, drag-and-drop transformation capabilities.
* A **Data Flow** is a **graphical data transformation** component in ADF

### üîÑ Common Transformations

| Transformation    | Purpose                                  |
|-------------------|------------------------------------------|
| Filter            | Include/exclude rows based on conditions |
| Join              | Combine data from two sources            |
| Derived Column    | Create new columns using expressions     |
| Aggregate         | Perform groupings and aggregations       |
| Conditional Split | Route data to different paths            |
| Pivot/Unpivot     | Reshape tabular data                     |
| Surrogate Key     | Add unique identifiers                   |
| Lookup            | Reference data from another source       |

```json
{
  "name": "CleanSalesDataFlow",
  "type": "MappingDataFlow",
  "typeProperties": {
    "sources": [...],
    "transformations": [...],
    "sinks": [...]
  }
}
```

---

### üìä Summary Table

| Component           | Purpose                                 |
|---------------------|------------------------------------------|
| Pipeline            | Container of activities                  |
| Activity            | Task like Copy, Lookup, Execute Notebook |
| Dataset             | Input/output data descriptor             |
| Linked Service      | Data source connection                   |
| Trigger             | Defines execution schedule               |
| Integration Runtime | Compute engine for pipeline execution    |
| Parameters          | Makes logic dynamic and reusable         |
| Data Flow           | GUI-based data transformation logic      |

---


