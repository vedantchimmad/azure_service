# üì¶ Copy Activity in Azure Data Factory (ADF)

---

## üìå Introduction
The **Copy Activity** is one of the most commonly used activities in **Azure Data Factory (ADF)**.  
It is designed to **copy data from a source (input dataset) to a destination (output dataset)**.  
It supports a wide range of **data stores** such as **Azure Blob, Data Lake, SQL, Cosmos DB, On-premises DBs, SaaS apps, and REST APIs**.

---

## ‚öôÔ∏è How It Works
1. **Connects to a Source** ‚Üí Reads data from the input dataset.  
2. **Performs Serialization/Deserialization** ‚Üí Converts data into required format (CSV, JSON, Parquet, Avro, ORC, etc.).  
3. **Writes to Sink** ‚Üí Loads the data into the target store.  

---

## üõ†Ô∏è Syntax (JSON Pipeline Definition Example)
```json
{
  "name": "CopyFromBlobToSQL",
  "properties": {
    "activities": [
      {
        "name": "CopyBlobToSQL",
        "type": "Copy",
        "inputs": [
          { "referenceName": "BlobDataset", "type": "DatasetReference" }
        ],
        "outputs": [
          { "referenceName": "SQLDataset", "type": "DatasetReference" }
        ],
        "typeProperties": {
          "source": { "type": "BlobSource" },
          "sink": { "type": "SqlSink" }
        }
      }
    ]
  }
}
````

---

## üîë Key Components

| Component                    | Description                                                  |
| ---------------------------- | ------------------------------------------------------------ |
| **Source**                   | Defines the input dataset (e.g., Blob, SQL, REST API).       |
| **Sink**                     | Defines the output dataset (e.g., SQL, ADLS, Synapse).       |
| **Integration Runtime (IR)** | Compute used for data movement (Azure IR or Self-hosted IR). |
| **Mapping**                  | Maps source schema to sink schema (optional).                |
| **Performance Settings**     | Batch size, parallelism, fault tolerance.                    |

---

## ‚ö° Supported Source & Sink Types

* **File-based**: Blob, ADLS, FTP, SFTP, Amazon S3, Google Cloud Storage.
* **Databases**: Azure SQL, SQL Server, MySQL, PostgreSQL, Oracle, Snowflake.
* **NoSQL**: Cosmos DB, MongoDB, Cassandra.
* **SaaS Apps**: Salesforce, Dynamics 365, ServiceNow, SAP.
* **REST & OData APIs**.

---

## üöÄ Example: Copy CSV from Blob ‚Üí Azure SQL Database

### 1Ô∏è‚É£ Create Linked Services

* **Azure Blob Storage** (source).
* **Azure SQL Database** (sink).

### 2Ô∏è‚É£ Create Datasets

* Input: Blob (CSV).
* Output: Azure SQL table.

### 3Ô∏è‚É£ Pipeline with Copy Activity

```json
{
  "name": "BlobToSQLPipeline",
  "properties": {
    "activities": [
      {
        "name": "CopyBlobToSQL",
        "type": "Copy",
        "inputs": [
          { "referenceName": "BlobDataset", "type": "DatasetReference" }
        ],
        "outputs": [
          { "referenceName": "SQLDataset", "type": "DatasetReference" }
        ],
        "typeProperties": {
          "source": { "type": "DelimitedTextSource" },
          "sink": { "type": "SqlSink" },
          "enableStaging": false
        }
      }
    ]
  }
}
```

---

## üìä Performance Optimization

* Enable **parallel copy** for large datasets.
* Use **staging copy** with Azure Blob or ADLS for transformations.
* Configure **fault tolerance** (skip incompatible rows, log errors).
* Tune **batch size** and **max concurrent connections**.

---

## üéØ Key Takeaways

* **Copy Activity** is the foundation of ETL pipelines in ADF.
* Supports **100+ data sources and sinks**.
* Handles **schema mapping, format conversion, fault tolerance, and performance tuning**.
* Can be used with **triggers** for **batch or real-time ingestion**.

---
